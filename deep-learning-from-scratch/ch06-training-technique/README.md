# Learning
メモ
* パラメータを更新する方法には、SGD(確率的勾配降下法), Momentum, AdaGrad, Adamがある
  * SGDは実装がシンプル。探索経路が非効率
  * MomentumはSGDに比べて効率的に最小値へ向かう。ボールが地面の傾斜を転がるように動くイメージ
  * AdaGradはパラメータ要素ごとに更新ステップを調整できる
  * AdamはMomentumとAdaGradを融合した手法
* 重みの初期値を均一の値にしてしまうと、誤差逆伝搬ですべての重みの値が均一に統一されてしまうので、ランダムな初期値が必要
* 隠れ層のアクティベーション(活性化関数のあとの出力データ)の分布は適度な広がりがあると、多様性のあるデータを表現することができる。重みの初期値として、Xavierの初期値を用いると広がりを持った分布になって効率的に学習が行える
* 重みの初期値として、標準偏差が0.01のガウス分布、Xavierの初期値、Heの初期値を比べると、標準偏差が0.0.1のガウス分布では順伝搬で小さな値が流れるために全く学習できていない
* 強制的にアクティベーションの分布を調整する手法がBatch Normalization。データ分布の正規化を行うレイヤとしてBatch Normレイヤを挿入する。ほとんどすべてのケースで学習が高速化される
* 過学習抑制のためにWeight decayやDropoutという手法がよく使われる
* ハイパーパラメータ最適化のステップ
  * ステップ0: ハイパーパラメータの範囲を設定する
  * ステップ1: 設定されたパラメータの範囲から、ランダムにサンプリングする
  * ステップ2: ステップ1でサンプリングされたハイパーパラメータの値を使用して学習を行い、検証データで認識精度を評価する(ただし、エポック回数は小さく設定)
  * ステップ3: ステップ1とステップ2をある回数(100回など)繰り返し、それらの認識精度の結果から、ハイパーパラメータの範囲を狭める

