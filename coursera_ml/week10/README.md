# LARGE SCALE MACHINE LEARNING

## Gradient Descent with Large Datasets

#### Learning With Large Datasets

* 大規模なデータセットでは計算量が多くなる

* トレーニングセットのサイズm=100,000,000の最急降下法を計算するときには、ワンステップを計算するのに1億の和を計算する必要がある

* 大量のデータセットを使う前に、1000くらいのデータセットでトレーニングするのははダメなのかを考える必要がある

* より少ないデータセットでサニティチェックを行うには、学習曲線をプロットしてみる
  * 高バリアンスならデータセットを更に追加する必要あり
  * 高バイアスならあまりデータを追加しても良くはならない

#### Stochastic Gradient Descent

* 大量にトレーニングセットがあるときは最急降下法では計算量が多すぎるので確率的最急降下法を使う

* 最急降下法(バッチ最急降下法)では一度すべてのトレーニングの手本を見る必要がある

* 確率的最急降下法では1回のイテレーションで1つのトレーニングセットだけ見れば良い

* 確率的最急降下法のコスト関数。コスト関数の項は、仮説が単体の手本x(i)とy(i)に対してどれだけ良いかを測っている

![cost function](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-1.png)

* Jtrainはm個のトレーニング手本の仮説のコストの平均

![Jtrain](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-2.png)

* 確率的最急降下法のアルゴリズム
  1. データセットをランダムにシャッフルする。m個のトレーニングサンプルをランダムに並べ替える
  2. トレーニング手本のアップデートを全てのjに対して実行を繰り返す。微分項を一つのトレーニング手本に対してだけ取る
    * ![Update Theta](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-3.png)
 
* トレーニングセットの全てを足し合わせる必要がないので、確率的最急降下法のイテレーションはバッチ最急降下法と比べて早い。その分グローバル最小に遠回りの軌跡をたどって進んでします。最終的にはグローバル最小の周りの範囲でうろうろする

* ループは1回から10回くらい繰り返せばよい。トレーニングセットの大きさによる

* 確率的最急降下法は線形回帰アルゴリズムに依らず、ロジスティック回帰やニューラルネットワークにも使える

#### Mini-Batch Gradient Descent

* バッチ最急降下法は全ての手本、確率的最急降下法は1つの手本を使うのに対して、ミニバッチ最急降下法はイテレーションでb個の手本を使う

* ミニバッチのサイズbは10とか2-100の範囲で選ぶべき

* b=10のときのθをアップデートする式

* ![update theta at mini-batch](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-4.png)

* b=10でm=1000のとき、for i= 1, 11, 21, 31,..., 991のように繰り返す

* よいベクトル実装があるときにおいて、ミニバッチ最急降下法は確率的最急降下法と比べて並列に処理できるので処理が早くなる。良いベクトル実装がない場合は、計算量が増えて遅くなってしまう

#### Stochastic Gradient Descent Convergence

* 確率的最急降下法が収束しているか、αのチューニング方法

* バッチ最急降下法ではイテレーションに対してコスト関数をプロットして値が減少しているかどうかで確認していた

* 確率的最急降下法が収束しているかを確認する方法
  1. xi, yiがθをアップデートする前に、そのトレーニング手本に対して仮説がどれだけ良いかを計算する。θをアップデートした後では、その手本については代表的な値よりももっと良くなってしまうため、アップデートの前に実行する
  2. 1000のイテレーションを繰り返すごとに、その前のステップで計算したこれらのコスト関数をプロットする。アルゴリズムに処理された最後の1000手本に渡るコストの平均をプロットできる

* プロットの例。プロットはノイジーになる
  * αが小さいと、収束した時にコストは小さくなるが収束に時間がかかる(左上のグラフ)
  * 手本を増やして平均をとると、よりスムースになるが、計算に時間が掛かるが遅くなる(右上のグラフ)
  * ノイジーでコストが減少していないように見えても、手本を増やすことによって収束することもある(左下のグラフ)
  * イテレーションを増やすに連れてコストが増加しているときはアルファを小さくする(右下のグラフ)

![checking for convergence](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-5.png)

* αは定数のまま保つのが基本。グローバル最小に収束させたいときは、時間が経つにつれてゆっくりアルファを減少させる。以下は、αをイテレーションが増えるに連れて減少させるためによく使われる式

![decrease alpha](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-1-6.png)

## Advanced Topics

#### Online Learning

* オンライン機械学習によって、データのストリームからユーザの嗜好を学習して、意思決定を最適化するのに使うことができる

* 配送サービスのwebサイトで、ユーザが配達元と配達先を選んで、ユーザに値段を提示し、それに基づいてユーザがサービスを使うか(y=1)使わないか(y=0)の例で、ユーザにオファーしたい価格を最適化するアルゴリズムを考える
  * ユーザのフィーチャー(年齢とかどこに送りたいかなど)を使って、ユーザが使ってくれる確率(p(y=1|x;θ))を学習して最適な値段を提示する
  * ロジスティック回帰やニューラルネットワークを用いる
  * オンライン学習において、手本を取り出したらその手本を使って学習を行い、その手本を捨て去ってしまう。データが多い時に有効

* 商品検索のアプリケーションで、ユーザに良い検索結果のリストを与える機械学習アルゴリズム
  * ストアにある100の携帯電話から10の検索結果を返すアルゴリズムがほしい
  * 携帯電話と特定のユーザのクエリが与えられたとして、フィーチャーベクトルを構築する事ができる。どのくらいのワードが携帯電話の名前にマッチしたか、どのくらいのワードが携帯電話の概要にマッチしたか
  * ユーザが各携帯電話に対してクリックする確率(p(y=1|x;θ))を求めたい。ユーザがクリックすればy=1しなければy=0
  * この確率の上位10件を表示
 
* 他のオンライン学習が使える例
  * ユーザに対して特別にオファーするものを選択
  * カスタマイズされたニュース記事の選択
  * 製品のレコメンド

* ユーザの嗜好が変化していく時も、オンライン学習でゆっくり適応することができる

#### Map Reduce and Data Parallelism

* 大規模な機械学習を行う上で使われるMap Reduceアルゴリズム

* 複数台のサーバにサブセット全体の一部を処理させ、それらを足しあわせて中央サーバで計算。この足しあわせた結果は最急降下法と等価となる

![update Theta](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-2-1.png)

* MapReduceのテクニックの図

![map reduce](https://github.com/wkodate/CourseraML/blob/master/week10/images/week10-2-2.png)

* MapReduceを適用する場合に検討するのは、使う学習アルゴリズムはトレーニングセットに渡る和の形で表現できるかどうか

* マルチプロセッサコアのマシンの場合、トレーニングセットをコア数に分割してワークロードを分散できる。ネットワークのレイテンシを考える必要なし

