## Clustering

#### Unsupervised Learning: Introduction

* 教師あり学習ではラベル付きのトレーニングセットが与えられてそれを仮説にフィットさせる

* 教師なし学習ではトレーニングセットにラベルがない。x(1), x(2)...だけでy(1), y(2)...がない。クラスタリングアルゴリズムを使う。クラスタリングは教師なし学習の一例であり、イコールではない

* クラスタリングの利用例
  * マーケットセグメンテーション
  * SNS解析
  * コンピューティングクラスタの構成
  * 天文学データの解析

#### K-Means Algorithm

* K-Meansアルゴリズムは有名なクラスタリングアルゴリズム

* K-Meansアルゴリズム
  * ランダムにK個(K: グループ分けしたい数)のクラスタの重心を選ぶ(μ1, μ2,...μK)
  * **クラスタ割り当てステップ(Cluster Assignment Step)**
    * m個の各点がどちらの重心と近いかを分ける。min||x(i) - μk||^2 でi番目のトレーニング手本とクラスタ重心μkとの距離を最小化するc(i)を決定。
  * **重心移動ステップ(Move Centroid Step)**
    * 割り当てた各点で新しい重心を求め、k個の重心をそれぞれ移動させる。
  * クラスタ割り当てステップと重心移動ステップを繰り返し、これ以上移動しない点まで移動させる。c(k)=kとなったx(i)の平均からn次元ベクトルのμkを決定

*  一つも割り当てられない重心が出た場合は、その重心を消してしまうか、重心の再割当てをする

* K-Meansを使うと、グルーピングしにくい性の相関を持ったようなデータでも複数のクラスタに分けてくれる。身長と体重の分布から、SMLのサイズのTシャツに分けるマーケットセグメンテーションの例

#### Optimization Objective

* K-meansの記法
  * c(i): x(i)が現在割り当てられているクラスタのインデックス
  * μk: クラスタの重心Kの場所。n次元ベクトル
  * μc(i): クラスタの重心のうち、x(i)に割り当てられているもの

* コスト関数

![cost function](https://github.com/wkodate/CourseraML/blob/master/week8/images/week8-1-1.png)

* トレーニングサンプルとクラスタの重心の二乗誤差を最小化

* コスト関数を用いたK-meansアルゴリズム
  * Kのランダム初期化
  * クラスタ割り当て
    * μ1,...,μkを固定し、c(1),...c(n)に関してJを最小化
  * 重心移動  
    * μ1,...,μkに関してJを最小化
  * 繰り返す

#### Random Initialization

* 最適な初期化の方法
  * クラスタ数Kをトレーニング手本mの数よりも小さくする
  * K個のトレーニング手本をランダムに選ぶ
  * これらK個の手本をクラスタ重心μ1,...μKにセットする

* 局所最適な解となってしまう場合、K-meanがあるのでK-meansを何度か実行する必要がある(50-100回)
  * 複数回実行したデータから一番低いコストを選択する
  * クラスタ総数Kが小さいときほどやるべき

#### Choosing the Number of Clusters

* クラスタ総数Kを選択する方法の一つであるエルボー法(Elbow method)
  * Kを変化させたときのコスト関数Jを求めてプロット。
  * Kに対するコスト関数のグラフは、左腕のようなグラフになる。Kを1(肩)から増やしていくとJが減少し、あるK(肘)からゆるやかに減少するようになる。Kの増加させた先が手を表す。この肘に当たるKが選ぶべきクラスタ数
  * 連続的にあいまいに減少する場合はあまりわからない
* ベストな方法ではないので期待しない方がいい

* どういう目的でK-meansを実行しているかの観点からクラスタ総数を決定する

## Motivation

#### Motivation I: Data Compression

* 次元削減により冗長性を削減。相関があるフィーチャをまとめる

* 2D→1D. 元のデータセットで引いた直線の射影を直線上に表す

* 3D→2D, 平面に射影して二次元ベクトルに変換

#### Motivation II: Visualization

* 2次元に次元削減することによってデータをプロットして可視化する


## Principal Component Analysis

#### Principal Component Analysis Problem Formulation

* 主成分分析(PCA: Principal Component Analysis)

* 射影誤差が小さくなるような直線を引く

* N次元のデータをk次元へと削減したいとき、射影先の点と射影元の点との距離を最小化するような射影方向u(1),...,u(k)を見つけるために、斜影誤差が最小となるようなベクトルを探す

* PCAは線形回帰ではない
  * 線形回帰
    * 点と直線の距離による二乗誤差が最小となるような直線をフィッティングする
    * 入力xに対するyを求めるので、サンプルと直線は直行していない。垂直距離
  * PCA
    * 大きさを最小化したいのでサンプルと直線は直行した最短距離となる
    * 予測したいyは存在せず、x1,x2,,...xnは等しく扱われる

#### Principal Component Analysis Algorithm

* PCAのアルゴリズムでn次元をk次元に削減する
  * 平均標準化とフィーチャースケーリングを行う
  * 共分散行列Σを求める
    * ![Compute covariance matrix](https://github.com/wkodate/CourseraML/blob/master/week8/images/week8-3-1.png)
    * octaveだとSigma = (1/m)*X'*Xj
  * 共分散行列Σ(n×n行列)のeigenvectorsを求める
    * octaveではsvd関数を用いる。返り値としてU,S,Vの行列が得られる
    * eig関数でも可
    * SVDは特異値分散(Singular Value Decomposition)の略
  *U,S,VからUが得られる。Uはn×n行列。
  * Uからk列分を取り出して次元削減。この削減したUreduceはn×k行列
  * 削減した結果のzはこのUreduceの転置行列にトレーニングセットXをかけたもの

* PCAアルゴリズムのまとめ
  * 平均標準化、フィーチャースケーリング
  * 各フィーチャの平均を計算
  * Sigma計算。ベクトライズできる
  * 得られたUの最初からk列までをUreduceとする
  * UreduceとフィーチャーベクトルXから変換してzを求める

## Applying PCA

#### Reconstruction from Compressed Representation

* 圧縮したデータを元の次元に近似させて戻す方法

![Xapprox](https://github.com/wkodate/CourseraML/blob/master/week8/images/week8-4-1.png)

#### Choosing the Number of Principal Components

* 主成分の数Kを選ぶ方法として、平均二乗射影誤差/データ全体の分散 が1%未満になるようなkを選ぶ。分散の99%が保持されている状態

![choose k](https://github.com/wkodate/CourseraML/blob/master/week8/images/week8-4-2.png)

* kを選択するアルゴリズム
  * k=1とする
  * Ureduceを計算
  * 分散が99%保持されているか確認
  * 保持されていればｋ=1を使う。保持されていなければkをインクリメントしてで最初から繰り返す

* kを選択するアルゴリズムの簡単な方法(上記アルゴリズムの反復が必要なくなる)
  * 共分散に対してsvd関数を実行して得られるU,S,Vを得る
  * Sの対角行列を使って以下の式のkが最も小さくなる値を見つける

![choose k](https://github.com/wkodate/CourseraML/blob/master/week8/images/week8-4-3.png)

#### Advice for Applying PCA

* 教師あり学習をスピードアップさせる
  * 入力から高次元のラベルがついていないデータセットを取り出して、PCAで次元削減
  * 次元削減して得られたzとラベルを新しいトレーニングセットとして仮説に与える
  * PCAはトレーニングセットにだけのみ適用させる。クロスバリデーションセットやテストセットには使用してはいけない

* PCAの応用
  * 圧縮
    * メモリやディスクの削減
    * 学習アルゴリズムの高速化
  * 可視化(2次元や3次元にする)

* PCAを使ってはいけない例
  * オーバーフィッティングを解消するために利用する
    * 次元削減の代わりに正規化を使う。ラベルの情報を持っていないので重要な情報を捨て去ってしまう可能性があるから
  * MLシステムの設計で
    * PCAなしでオリジナルの生データx(i_を使ってまずは実行してみる。それでうまくいかない場合に始めてPCAでz(i)を使って実行してみる
